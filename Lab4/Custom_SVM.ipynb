{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import functools\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(527, 40)\n",
      "First few data rows:\n",
      "[['D-1/3/90' 44101.0 1.5 7.8 nan]\n",
      " ['D-2/3/90' 39024.0 3.0 7.7 nan]\n",
      " ['D-4/3/90' 32229.0 5.0 7.6 nan]\n",
      " ['D-5/3/90' 35023.0 3.5 7.9 205.0]\n",
      " ['D-6/3/90' 36924.0 1.5 8.0 242.0]\n",
      " ['D-7/3/90' 38572.0 3.0 7.8 202.0]\n",
      " ['D-8/3/90' 41115.0 6.0 7.8 nan]\n",
      " ['D-9/3/90' 36107.0 5.0 7.7 215.0]\n",
      " ['D-11/3/90' 29156.0 2.5 7.7 206.0]\n",
      " ['D-12/3/90' 39246.0 2.0 7.8 172.0]]\n",
      "Num samples == 527\n",
      "Num features == 38\n"
     ]
    }
   ],
   "source": [
    "inputFile = \"water-treatmennt-original-marked.csv\"\n",
    "\n",
    "dataFrame = pd.read_csv(inputFile, header = 0, sep = ';')\n",
    "print(dataFrame.shape)\n",
    "data = dataFrame.values\n",
    "\n",
    "print(\"First few data rows:\")\n",
    "print(data[0:10, 0:5])\n",
    "\n",
    "numSamples = dataFrame.shape[0]\n",
    "print(\"Num samples == \" + str(numSamples))\n",
    "numFeatures = dataFrame.shape[1] - 2  #first feature is sampling date and last feature used as class marker\n",
    "print(\"Num features == \" + str(numFeatures))\n",
    "\n",
    "regularizationStrength = 1\n",
    "learningRate = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 13.0\n",
      "   Q-E      (input flow to plant)   ZN-E     (input Zinc to plant)\n",
      "0                         44101.0                              1.5\n",
      "1                         39024.0                              3.0\n",
      "2                         32229.0                              5.0\n",
      "3                         35023.0                              3.5\n",
      "4                         36924.0                              1.5\n",
      "5                         38572.0                              3.0\n",
      "6                         41115.0                              6.0\n",
      "7                         36107.0                              5.0\n",
      "8                         29156.0                              2.5\n",
      "9                         39246.0                              2.0\n",
      "          0         1\n",
      "0  0.680598  0.041916\n",
      "1  0.579121  0.086826\n",
      "2  0.443305  0.146707\n",
      "3  0.499151  0.101796\n",
      "4  0.537147  0.041916\n",
      "5  0.570087  0.086826\n",
      "6  0.620915  0.176647\n",
      "7  0.520817  0.146707\n",
      "8  0.381883  0.071856\n",
      "9  0.583558  0.056886\n"
     ]
    }
   ],
   "source": [
    "##########################################Removal of useless features#####################\n",
    "\n",
    "Y = dataFrame.iloc[:, -1]  # Class markers\n",
    "X = dataFrame.iloc[:, 1:-1]  # Features without date and class markers\n",
    "\n",
    "numClasses = np.amax(Y)\n",
    "print(\"Num classes: {0}\".format(numClasses))\n",
    "\n",
    "print(X.iloc[0:10, 0:2])\n",
    "\n",
    "##########################################Normalization#####################\n",
    "\n",
    "normalizedX = MinMaxScaler().fit_transform(X)\n",
    "X = pd.DataFrame(normalizedX)\n",
    "\n",
    "print(X.iloc[0:10, 0:2])\n",
    "\n",
    "##########################################Missing data handling#####################\n",
    "\n",
    "for sampleId in range(X.shape[0]):\n",
    "    for featureId in range(numFeatures):\n",
    "        if (math.isnan(X.iloc[sampleId, featureId])):\n",
    "            X.iloc[sampleId, featureId] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFeatureForBias(data):\n",
    "    extendedData = np.zeros((data.shape[0], data.shape[1] + 1))\n",
    "    extendedData[:, 0:-1] = data\n",
    "    extendedData[:, -1] = int(1)\n",
    "    \n",
    "    return extendedData\n",
    "\n",
    "# test = pd.DataFrame([[0, 0],\n",
    "#                      [0, 0]])\n",
    "# print(addFeatureForBias(test))\n",
    "\n",
    "X = addFeatureForBias(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSoftMarginCost(weights, X, Y):\n",
    "    # Hinge loss\n",
    "    N = X.shape[0]\n",
    "    distances = 1 - Y * (np.dot(X, weights))\n",
    "    \n",
    "    numClassificationErrors = len(distances[distances < 0])\n",
    "    \n",
    "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "    \n",
    "    dangerousDistances = distances[distances < 1]\n",
    "    \n",
    "    numClassificationWarnings = len(dangerousDistances[dangerousDistances > 0])\n",
    "    \n",
    "    hinge_loss = regularizationStrength * (np.sum(distances) / N)\n",
    "    \n",
    "    # Soft margin cost function\n",
    "    cost = 1 / 2 * np.dot(weights, weights) + hinge_loss\n",
    "    return cost, numClassificationWarnings, numClassificationErrors\n",
    "\n",
    "def computeCostFunctionGradient(weights, xBatch, yBatch):\n",
    "#     print(type(xBatch))\n",
    "#     print(type(yBatch))\n",
    "    \n",
    "    # In case of SGD\n",
    "    if (type(yBatch) != np.ndarray):\n",
    "        yBatch = np.array([yBatch])\n",
    "        xBatch = np.array([xBatch])\n",
    "        \n",
    "    distances = 1 - (yBatch * np.dot(xBatch, weights))\n",
    "    \n",
    "    weightsDelta = np.zeros(len(weights))\n",
    "    \n",
    "    for index, distance in enumerate(distances):\n",
    "        if (max(0, distance) == 0):\n",
    "            deltaI = weights\n",
    "        else:\n",
    "            deltaI = weights - (regularizationStrength * yBatch[index] * xBatch[index])\n",
    "            \n",
    "        weightsDelta += deltaI\n",
    "        \n",
    "    weightsDelta = weightsDelta / len(yBatch)\n",
    "    \n",
    "    return weightsDelta\n",
    "\n",
    "def stochasticGradientDescent(features, outputs):\n",
    "    maxEpochs = 1024\n",
    "    weights = np.zeros(features.shape[1])\n",
    "\n",
    "    prevCost = float(\"inf\")\n",
    "    convergenceThreshold = 0.01 #percents\n",
    "    \n",
    "    classificationWarnings = 0\n",
    "    classificationErrors = 0\n",
    "    \n",
    "    for epoch in range(maxEpochs): \n",
    "        # To prevent repeating update cycles\n",
    "        X, Y = shuffle(features, outputs)\n",
    "        \n",
    "        for ind, x in enumerate(X):\n",
    "            gradientAscent = computeCostFunctionGradient(weights, x, Y[ind])\n",
    "            weights = weights - (learningRate * gradientAscent)\n",
    "            \n",
    "        if ((epoch % 100 == 0) or (epoch >= maxEpochs - 1)):\n",
    "            cost, classificationWarnings, classificationErrors = computeSoftMarginCost(weights, \n",
    "                                                                                       features, \n",
    "                                                                                       outputs)\n",
    "            \n",
    "            print(\"Epoch #{0}: cost == {1}\".format(epoch, cost))\n",
    "            print(\"Epoch #{0}: Num classification errors == {1}\".format(epoch, classificationErrors))\n",
    "            print(\"Epoch #{0}: Num samples inside borders == {1}\".format(epoch, classificationWarnings))\n",
    "            \n",
    "            if (abs(prevCost - cost) < convergenceThreshold * prevCost):\n",
    "                return weights, classificationWarnings, classificationErrors\n",
    "            \n",
    "            prevCost = cost\n",
    "            \n",
    "    return weights, classificationWarnings, classificationErrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyOneVsAll(oneClassId):\n",
    "    yOneVsAll = list(map(lambda y: -1.0 if y == oneClassId else 1.0, Y))\n",
    "\n",
    "    # print(yFirstVsAll)\n",
    "    # print(type(X.values))\n",
    "    # print(type(yFirstVsAll))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, yOneVsAll, test_size=0.3, random_state=42)\n",
    "\n",
    "    print(\"training started...\")\n",
    "    print(\"Num samples: {0}\".format(X_train.shape[0]))\n",
    "    \n",
    "    W, warnings, errors = stochasticGradientDescent(X_train, y_train)\n",
    "    \n",
    "    if (errors > 0):\n",
    "        return False\n",
    "    \n",
    "    print(\"training finished.\")\n",
    "    print(\"weights are: {}\".format(W))\n",
    "\n",
    "    y_test_predicted = [0] * X_test.shape[0]\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        yp = np.sign(np.dot(W, X_test[i]))\n",
    "        y_test_predicted[i] = yp\n",
    "\n",
    "    # print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Trying to split Class#1 from other classes...\n",
      "training started...\n",
      "Num samples: 368\n",
      "Epoch #0: cost == 0.9999740688324595\n",
      "Epoch #0: Num classification errors == 0\n",
      "Epoch #0: Num samples inside borders == 195\n",
      "Epoch #100: cost == 0.9974751021860854\n",
      "Epoch #100: Num classification errors == 0\n",
      "Epoch #100: Num samples inside borders == 195\n",
      "training finished.\n",
      "weights are: [-6.01799537e-04  1.09237194e-04 -7.76653774e-04 -1.39961936e-03\n",
      " -9.97577076e-04  5.09797337e-04 -3.92331636e-03  6.96262297e-05\n",
      " -8.67018924e-04 -4.29643330e-04 -1.13303020e-03  5.99184078e-04\n",
      " -3.44773987e-03  4.89132557e-05 -9.70287366e-04 -4.88114100e-04\n",
      " -2.27117745e-03 -2.89318617e-03 -3.90996794e-04 -2.91325748e-03\n",
      " -5.66545617e-06 -1.12985387e-03 -5.51117948e-06 -7.30931442e-04\n",
      " -9.71689789e-04 -4.33764053e-04 -2.15363047e-03 -3.39739822e-06\n",
      " -7.90907354e-04  7.21617169e-04 -8.51722783e-05 -2.23911989e-03\n",
      " -1.67460187e-03 -1.98355452e-03 -1.38753854e-03 -1.58955059e-03\n",
      " -1.35895611e-03 -2.21037476e-03 -2.18122189e-03]\n",
      ">>>Trying to split Class#2 from other classes...\n",
      "training started...\n",
      "Num samples: 368\n",
      "Epoch #0: cost == 0.9959982721079762\n",
      "Epoch #0: Num classification errors == 0\n",
      "Epoch #0: Num samples inside borders == 368\n",
      "Epoch #100: cost == 0.6103387933547224\n",
      "Epoch #100: Num classification errors == 0\n",
      "Epoch #100: Num samples inside borders == 368\n",
      "Epoch #200: cost == 0.25204443918425373\n",
      "Epoch #200: Num classification errors == 0\n",
      "Epoch #200: Num samples inside borders == 368\n",
      "Epoch #300: cost == 0.06728303140421792\n",
      "Epoch #300: Num classification errors == 248\n",
      "Epoch #300: Num samples inside borders == 120\n",
      "Epoch #400: cost == 0.062185026345761854\n",
      "Epoch #400: Num classification errors == 304\n",
      "Epoch #400: Num samples inside borders == 64\n",
      "Epoch #500: cost == 0.061555616054407805\n",
      "Epoch #500: Num classification errors == 318\n",
      "Epoch #500: Num samples inside borders == 50\n",
      "Epoch #600: cost == 0.061444810561710785\n",
      "Epoch #600: Num classification errors == 323\n",
      "Epoch #600: Num samples inside borders == 45\n",
      ">>>Classification of Class#2 vs All failed\n",
      ">>>Trying to split Class#3 from other classes...\n",
      "training started...\n",
      "Num samples: 368\n",
      "Epoch #0: cost == 0.9959982678314763\n",
      "Epoch #0: Num classification errors == 0\n",
      "Epoch #0: Num samples inside borders == 368\n",
      "Epoch #100: cost == 0.6103388084190181\n",
      "Epoch #100: Num classification errors == 0\n",
      "Epoch #100: Num samples inside borders == 368\n",
      "Epoch #200: cost == 0.25204449433548193\n",
      "Epoch #200: Num classification errors == 0\n",
      "Epoch #200: Num samples inside borders == 368\n",
      "Epoch #300: cost == 0.06727336997434913\n",
      "Epoch #300: Num classification errors == 248\n",
      "Epoch #300: Num samples inside borders == 120\n",
      "Epoch #400: cost == 0.062183665863902324\n",
      "Epoch #400: Num classification errors == 304\n",
      "Epoch #400: Num samples inside borders == 64\n",
      "Epoch #500: cost == 0.061555396579396945\n",
      "Epoch #500: Num classification errors == 318\n",
      "Epoch #500: Num samples inside borders == 50\n",
      "Epoch #600: cost == 0.06144476670133187\n",
      "Epoch #600: Num classification errors == 323\n",
      "Epoch #600: Num samples inside borders == 45\n",
      ">>>Classification of Class#3 vs All failed\n",
      ">>>Trying to split Class#4 from other classes...\n",
      "training started...\n",
      "Num samples: 368\n",
      "Epoch #0: cost == 0.996095401585153\n",
      "Epoch #0: Num classification errors == 0\n",
      "Epoch #0: Num samples inside borders == 365\n",
      "Epoch #100: cost == 0.6197966963416225\n",
      "Epoch #100: Num classification errors == 0\n",
      "Epoch #100: Num samples inside borders == 365\n",
      "Epoch #200: cost == 0.2701989948332865\n",
      "Epoch #200: Num classification errors == 0\n",
      "Epoch #200: Num samples inside borders == 365\n",
      "Epoch #300: cost == 0.08067774918179738\n",
      "Epoch #300: Num classification errors == 238\n",
      "Epoch #300: Num samples inside borders == 127\n",
      "Epoch #400: cost == 0.07542431137348962\n",
      "Epoch #400: Num classification errors == 301\n",
      "Epoch #400: Num samples inside borders == 64\n",
      "Epoch #500: cost == 0.07486254946687586\n",
      "Epoch #500: Num classification errors == 313\n",
      "Epoch #500: Num samples inside borders == 52\n",
      ">>>Classification of Class#4 vs All failed\n",
      ">>>Trying to split Class#5 from other classes...\n",
      "training started...\n",
      "Num samples: 368\n",
      "Epoch #0: cost == 0.9988258679950336\n",
      "Epoch #0: Num classification errors == 0\n",
      "Epoch #0: Num samples inside borders == 282\n",
      "Epoch #100: cost == 0.8856706183823035\n",
      "Epoch #100: Num classification errors == 0\n",
      "Epoch #100: Num samples inside borders == 282\n",
      "Epoch #200: cost == 0.7805444033795209\n",
      "Epoch #200: Num classification errors == 0\n",
      "Epoch #200: Num samples inside borders == 282\n",
      "Epoch #300: cost == 0.6828777096074291\n",
      "Epoch #300: Num classification errors == 0\n",
      "Epoch #300: Num samples inside borders == 282\n",
      "Epoch #400: cost == 0.5921411966634699\n",
      "Epoch #400: Num classification errors == 0\n",
      "Epoch #400: Num samples inside borders == 282\n",
      "Epoch #500: cost == 0.5329212230878282\n",
      "Epoch #500: Num classification errors == 89\n",
      "Epoch #500: Num samples inside borders == 193\n",
      "Epoch #600: cost == 0.528960224129551\n",
      "Epoch #600: Num classification errors == 145\n",
      "Epoch #600: Num samples inside borders == 137\n",
      ">>>Classification of Class#5 vs All failed\n",
      ">>>Trying to split Class#6 from other classes...\n",
      "training started...\n",
      "Num samples: 368\n",
      "Epoch #0: cost == 0.9960840734407925\n",
      "Epoch #0: Num classification errors == 0\n",
      "Epoch #0: Num samples inside borders == 366\n",
      "Epoch #100: cost == 0.6186942734808736\n",
      "Epoch #100: Num classification errors == 0\n",
      "Epoch #100: Num samples inside borders == 366\n",
      "Epoch #200: cost == 0.2680828650297838\n",
      "Epoch #200: Num classification errors == 0\n",
      "Epoch #200: Num samples inside borders == 366\n",
      "Epoch #300: cost == 0.07897772614585635\n",
      "Epoch #300: Num classification errors == 240\n",
      "Epoch #300: Num samples inside borders == 126\n",
      "Epoch #400: cost == 0.07350423883895113\n",
      "Epoch #400: Num classification errors == 301\n",
      "Epoch #400: Num samples inside borders == 65\n",
      "Epoch #500: cost == 0.07286568064466564\n",
      "Epoch #500: Num classification errors == 313\n",
      "Epoch #500: Num samples inside borders == 53\n",
      ">>>Classification of Class#6 vs All failed\n",
      ">>>Trying to split Class#7 from other classes...\n",
      "training started...\n",
      "Num samples: 368\n",
      "Epoch #0: cost == 0.9959982691244859\n",
      "Epoch #0: Num classification errors == 0\n",
      "Epoch #0: Num samples inside borders == 368\n",
      "Epoch #100: cost == 0.610338774373318\n",
      "Epoch #100: Num classification errors == 0\n",
      "Epoch #100: Num samples inside borders == 368\n",
      "Epoch #200: cost == 0.2520444509815303\n",
      "Epoch #200: Num classification errors == 0\n",
      "Epoch #200: Num samples inside borders == 368\n",
      "Epoch #300: cost == 0.0672755477413544\n",
      "Epoch #300: Num classification errors == 248\n",
      "Epoch #300: Num samples inside borders == 120\n",
      "Epoch #400: cost == 0.062182552158262475\n",
      "Epoch #400: Num classification errors == 304\n",
      "Epoch #400: Num samples inside borders == 64\n",
      "Epoch #500: cost == 0.06155520863766105\n",
      "Epoch #500: Num classification errors == 318\n",
      "Epoch #500: Num samples inside borders == 50\n",
      "Epoch #600: cost == 0.06144484169326951\n",
      "Epoch #600: Num classification errors == 323\n",
      "Epoch #600: Num samples inside borders == 45\n",
      ">>>Classification of Class#7 vs All failed\n",
      ">>>Trying to split Class#8 from other classes...\n",
      "training started...\n",
      "Num samples: 368\n",
      "Epoch #0: cost == 0.9960426081823476\n",
      "Epoch #0: Num classification errors == 0\n",
      "Epoch #0: Num samples inside borders == 367\n",
      "Epoch #100: cost == 0.614655933383491\n",
      "Epoch #100: Num classification errors == 0\n",
      "Epoch #100: Num samples inside borders == 367\n",
      "Epoch #200: cost == 0.26033130710232266\n",
      "Epoch #200: Num classification errors == 0\n",
      "Epoch #200: Num samples inside borders == 367\n",
      "Epoch #300: cost == 0.07317342955303144\n",
      "Epoch #300: Num classification errors == 246\n",
      "Epoch #300: Num samples inside borders == 121\n",
      "Epoch #400: cost == 0.06791732900960018\n",
      "Epoch #400: Num classification errors == 302\n",
      "Epoch #400: Num samples inside borders == 65\n",
      "Epoch #500: cost == 0.06728237060512017\n",
      "Epoch #500: Num classification errors == 315\n",
      "Epoch #500: Num samples inside borders == 52\n",
      ">>>Classification of Class#8 vs All failed\n",
      ">>>Trying to split Class#9 from other classes...\n",
      "training started...\n",
      "Num samples: 368\n",
      "Epoch #0: cost == 0.997544679607032\n",
      "Epoch #0: Num classification errors == 0\n",
      "Epoch #0: Num samples inside borders == 325\n",
      "Epoch #100: cost == 0.760918533696005\n",
      "Epoch #100: Num classification errors == 0\n",
      "Epoch #100: Num samples inside borders == 325\n",
      "Epoch #200: cost == 0.5410824271901711\n",
      "Epoch #200: Num classification errors == 0\n",
      "Epoch #200: Num samples inside borders == 325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #300: cost == 0.3390781934724125\n",
      "Epoch #300: Num classification errors == 14\n",
      "Epoch #300: Num samples inside borders == 311\n",
      "Epoch #400: cost == 0.28972624815751263\n",
      "Epoch #400: Num classification errors == 213\n",
      "Epoch #400: Num samples inside borders == 112\n",
      "Epoch #500: cost == 0.28887308154468805\n",
      "Epoch #500: Num classification errors == 239\n",
      "Epoch #500: Num samples inside borders == 86\n",
      ">>>Classification of Class#9 vs All failed\n",
      ">>>Trying to split Class#10 from other classes...\n",
      "training started...\n",
      "Num samples: 368\n",
      "Epoch #0: cost == 0.9959982678717269\n",
      "Epoch #0: Num classification errors == 0\n",
      "Epoch #0: Num samples inside borders == 368\n",
      "Epoch #100: cost == 0.6103387683979867\n",
      "Epoch #100: Num classification errors == 0\n",
      "Epoch #100: Num samples inside borders == 368\n",
      "Epoch #200: cost == 0.25204441415569073\n",
      "Epoch #200: Num classification errors == 0\n",
      "Epoch #200: Num samples inside borders == 368\n",
      "Epoch #300: cost == 0.06727996459743996\n",
      "Epoch #300: Num classification errors == 248\n",
      "Epoch #300: Num samples inside borders == 120\n",
      "Epoch #400: cost == 0.06218536500873113\n",
      "Epoch #400: Num classification errors == 304\n",
      "Epoch #400: Num samples inside borders == 64\n",
      "Epoch #500: cost == 0.06155571323062927\n",
      "Epoch #500: Num classification errors == 318\n",
      "Epoch #500: Num samples inside borders == 50\n",
      "Epoch #600: cost == 0.06144482984958713\n",
      "Epoch #600: Num classification errors == 323\n",
      "Epoch #600: Num samples inside borders == 45\n",
      ">>>Classification of Class#10 vs All failed\n",
      ">>>Trying to split Class#11 from other classes...\n",
      "training started...\n",
      "Num samples: 368\n",
      "Epoch #0: cost == 0.9975336092946677\n",
      "Epoch #0: Num classification errors == 0\n",
      "Epoch #0: Num samples inside borders == 332\n",
      "Epoch #100: cost == 0.759838195631483\n",
      "Epoch #100: Num classification errors == 0\n",
      "Epoch #100: Num samples inside borders == 332\n",
      "Epoch #200: cost == 0.5390092970872105\n",
      "Epoch #200: Num classification errors == 0\n",
      "Epoch #200: Num samples inside borders == 332\n",
      "Epoch #300: cost == 0.3339361761149557\n",
      "Epoch #300: Num classification errors == 2\n",
      "Epoch #300: Num samples inside borders == 330\n",
      "Epoch #400: cost == 0.2737467952628105\n",
      "Epoch #400: Num classification errors == 217\n",
      "Epoch #400: Num samples inside borders == 115\n",
      "Epoch #500: cost == 0.2725502455843421\n",
      "Epoch #500: Num classification errors == 246\n",
      "Epoch #500: Num samples inside borders == 86\n",
      ">>>Classification of Class#11 vs All failed\n",
      ">>>Trying to split Class#12 from other classes...\n",
      "training started...\n",
      "Num samples: 368\n",
      "Epoch #0: cost == 0.9960402168435806\n",
      "Epoch #0: Num classification errors == 0\n",
      "Epoch #0: Num samples inside borders == 367\n",
      "Epoch #100: cost == 0.6144236235457576\n",
      "Epoch #100: Num classification errors == 0\n",
      "Epoch #100: Num samples inside borders == 367\n",
      "Epoch #200: cost == 0.2598852856552076\n",
      "Epoch #200: Num classification errors == 0\n",
      "Epoch #200: Num samples inside borders == 367\n",
      "Epoch #300: cost == 0.07306570735238777\n",
      "Epoch #300: Num classification errors == 247\n",
      "Epoch #300: Num samples inside borders == 120\n",
      "Epoch #400: cost == 0.06779004485554678\n",
      "Epoch #400: Num classification errors == 302\n",
      "Epoch #400: Num samples inside borders == 65\n",
      "Epoch #500: cost == 0.06714552187431395\n",
      "Epoch #500: Num classification errors == 315\n",
      "Epoch #500: Num samples inside borders == 52\n",
      ">>>Classification of Class#12 vs All failed\n",
      ">>>Trying to split Class#13 from other classes...\n",
      "training started...\n",
      "Num samples: 368\n",
      "Epoch #0: cost == 0.995998270241343\n",
      "Epoch #0: Num classification errors == 0\n",
      "Epoch #0: Num samples inside borders == 368\n",
      "Epoch #100: cost == 0.6103387856436753\n",
      "Epoch #100: Num classification errors == 0\n",
      "Epoch #100: Num samples inside borders == 368\n",
      "Epoch #200: cost == 0.2520444476015518\n",
      "Epoch #200: Num classification errors == 0\n",
      "Epoch #200: Num samples inside borders == 368\n",
      "Epoch #300: cost == 0.06727521813459524\n",
      "Epoch #300: Num classification errors == 248\n",
      "Epoch #300: Num samples inside borders == 120\n",
      "Epoch #400: cost == 0.062184650025651614\n",
      "Epoch #400: Num classification errors == 304\n",
      "Epoch #400: Num samples inside borders == 64\n",
      "Epoch #500: cost == 0.06155549122712998\n",
      "Epoch #500: Num classification errors == 318\n",
      "Epoch #500: Num samples inside borders == 50\n",
      "Epoch #600: cost == 0.06144482499816893\n",
      "Epoch #600: Num classification errors == 323\n",
      "Epoch #600: Num samples inside borders == 45\n",
      ">>>Classification of Class#13 vs All failed\n"
     ]
    }
   ],
   "source": [
    "for classId in range(1, int(numClasses) + 1):\n",
    "    print(\">>>Trying to split Class#{0} from other classes...\".format(classId))\n",
    "    if (False == classifyOneVsAll(classId)):\n",
    "        print(\">>>Classification of Class#{0} vs All failed\".format(classId))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
