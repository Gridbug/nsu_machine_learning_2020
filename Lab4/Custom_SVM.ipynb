{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import functools\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(527, 40)\n",
      "First few data rows:\n",
      "[['D-1/3/90' 44101.0 1.5 7.8 nan]\n",
      " ['D-2/3/90' 39024.0 3.0 7.7 nan]\n",
      " ['D-4/3/90' 32229.0 5.0 7.6 nan]\n",
      " ['D-5/3/90' 35023.0 3.5 7.9 205.0]\n",
      " ['D-6/3/90' 36924.0 1.5 8.0 242.0]\n",
      " ['D-7/3/90' 38572.0 3.0 7.8 202.0]\n",
      " ['D-8/3/90' 41115.0 6.0 7.8 nan]\n",
      " ['D-9/3/90' 36107.0 5.0 7.7 215.0]\n",
      " ['D-11/3/90' 29156.0 2.5 7.7 206.0]\n",
      " ['D-12/3/90' 39246.0 2.0 7.8 172.0]]\n",
      "Num samples == 527\n",
      "Num features == 38\n"
     ]
    }
   ],
   "source": [
    "inputFile = \"water-treatmennt-original-marked.csv\"\n",
    "\n",
    "dataFrame = pd.read_csv(inputFile, header = 0, sep = ';')\n",
    "print(dataFrame.shape)\n",
    "data = dataFrame.values\n",
    "\n",
    "print('First few data rows:')\n",
    "print(data[0:10, 0:5])\n",
    "\n",
    "numSamples = dataFrame.shape[0]\n",
    "print(\"Num samples == \" + str(numSamples))\n",
    "numFeatures = dataFrame.shape[1] - 2  #first feature is sampling date and last feature used as class marker\n",
    "print(\"Num features == \" + str(numFeatures))\n",
    "\n",
    "regularizationStrength = 10000\n",
    "learningRate = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Q-E      (input flow to plant)   ZN-E     (input Zinc to plant)\n",
      "0                         44101.0                              1.5\n",
      "1                         39024.0                              3.0\n",
      "2                         32229.0                              5.0\n",
      "3                         35023.0                              3.5\n",
      "4                         36924.0                              1.5\n",
      "5                         38572.0                              3.0\n",
      "6                         41115.0                              6.0\n",
      "7                         36107.0                              5.0\n",
      "8                         29156.0                              2.5\n",
      "9                         39246.0                              2.0\n",
      "          0         1\n",
      "0  0.680598  0.041916\n",
      "1  0.579121  0.086826\n",
      "2  0.443305  0.146707\n",
      "3  0.499151  0.101796\n",
      "4  0.537147  0.041916\n",
      "5  0.570087  0.086826\n",
      "6  0.620915  0.176647\n",
      "7  0.520817  0.146707\n",
      "8  0.381883  0.071856\n",
      "9  0.583558  0.056886\n"
     ]
    }
   ],
   "source": [
    "Y = dataFrame.iloc[:, -1]  # Class markers\n",
    "X = dataFrame.iloc[:, 1:-1]  # Features without date and class markers\n",
    "\n",
    "print(X.iloc[0:10, 0:2])\n",
    "\n",
    "# featureMaxValues = [0] * numFeatures\n",
    "# featureMinValues = [0] * numFeatures\n",
    "\n",
    "# for featureId in range(numFeatures):\n",
    "#     featureMaxValues[featureId] = np.amax(X[:, featureId])\n",
    "#     featureMaxValues[featureId] = np.amin(X[:, featureId])\n",
    "\n",
    "normalizedX = MinMaxScaler().fit_transform(X)\n",
    "X = pd.DataFrame(normalizedX)\n",
    "\n",
    "print(X.iloc[0:10, 0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFeatureForBias(data):\n",
    "    extendedData = np.zeros((data.shape[0], data.shape[1] + 1))\n",
    "    extendedData[:, 0:-1] = data\n",
    "    extendedData[:, -1] = int(1)\n",
    "    \n",
    "    return extendedData\n",
    "\n",
    "# test = pd.DataFrame([[0, 0],\n",
    "#                      [0, 0]])\n",
    "# print(addFeatureForBias(test))\n",
    "\n",
    "X = addFeatureForBias(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSoftMarginCost(weights, X, Y):\n",
    "    # Hinge loss\n",
    "    N = X.shape[0]\n",
    "    distances = 1 - Y * (np.dot(X, weights))\n",
    "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "    hinge_loss = regularizationStrength * (np.sum(distances) / N)\n",
    "    \n",
    "    # Soft margin cost function\n",
    "    cost = 1 / 2 * np.dot(weights, weights) + hinge_loss\n",
    "    return cost\n",
    "\n",
    "def computeCostFunctionGradient(weights, xBatch, yBatch):\n",
    "    # In case of SGD\n",
    "    if (type(yBatch) == np.float64):\n",
    "        yBatch = np.array([yBatch])\n",
    "        xBatch = np.array([xBatch])\n",
    "        \n",
    "    distances = 1 - (yBatch * np.dot(xBatch, weights))\n",
    "    \n",
    "    weightsDelta = np.zeros(len(weights))\n",
    "    \n",
    "    for index, distance in enumerate(distances):\n",
    "        if (max(0, distance) == 0):\n",
    "            deltaI = weights\n",
    "        else:\n",
    "            deltaI = weights - (regularizationStrength * yBatch[index] * xBatch[index])\n",
    "            \n",
    "        weightsDelta += deltaI\n",
    "        \n",
    "    weightsDelta /= len(yBatch)\n",
    "    \n",
    "    return weightsDelta\n",
    "\n",
    "def stochasticGradientDescent(features, outputs):\n",
    "    maxEpochs = 5000\n",
    "    weights = np.zeros(features.shape[1]) \n",
    "    \n",
    "    for epoch in range(maxEpochs): \n",
    "        # To prevent repeating update cycles\n",
    "#          X, Y = shuffle(features, outputs)\n",
    "        \n",
    "        for ind, x in enumerate(X):\n",
    "            gradientAscent = computeCostFunctionGradient(weights, x, Y[ind])\n",
    "            weights = weights - (learningRate * gradientAscent)\n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started...\n",
      "training finished.\n",
      "weights are: [0.04938529 0.00512588 0.04439552 0.03661628 0.03890413 0.00724016\n",
      " 0.06354364 0.01177256 0.01717806 0.03242038 0.0254247  0.00557694\n",
      " 0.0671211  0.00495743 0.01849328 0.04528484 0.03564538 0.04404416\n",
      " 0.03266971 0.06516236 0.013401   0.02868622 0.0247133  0.01045085\n",
      " 0.03336055 0.01733265 0.07566528 0.00253521 0.01466067 0.02797244\n",
      " 0.04056321 0.0852697  0.07347985 0.05733808 0.07931223 0.06709744\n",
      " 0.08106181 0.10030369 0.10305658 0.10305658]\n"
     ]
    }
   ],
   "source": [
    "print(\"training started...\")\n",
    "W = stochasticGradientDescent(X, Y)\n",
    "print(\"training finished.\")\n",
    "print(\"weights are: {}\".format(W))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
